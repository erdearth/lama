{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LAMA into the Wild Part1: DataPreperation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "In this section We will preprocess the Data that we needed in the whole program. The result will be stored in `../data/pre`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'data'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-0ae067df7260>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDATA_DIR\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlama\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mto_tuple\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midentity\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlama\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecorators\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0menable_logging\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'data'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gc\n",
    "import typing as t\n",
    "import logging\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from data import DATA_DIR\n",
    "from lama.util import to_tuple, identity\n",
    "from lama.util.decorators import enable_logging\n",
    "from lama.util.StreamerBuilder import StreamerBuilder\n",
    "from lama.preprocessing.DataProcessor import nans, change_object_col, reformat_dataframe, split_with_index, standarize_col"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.1 Define Global Constants\n",
    "\n",
    "To make our reading and writing more easier, we decide to define some global constants below, you can change them if you'd like to write to another location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUT_DIR = os.path.join(DATA_DIR, \"pre\")\n",
    "logger = logging.getLogger('root')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The process looks alike, we will handle them one by one.\n",
    "\n",
    "- filter out Nan values\n",
    "- check unique columns\n",
    "- convert object values to numericals\n",
    "- catagorize discrete and continous features\n",
    "- scaling columns with Standardizer or Normalizer. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and Test\n",
    "\n",
    "We will first handle train and test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv(os.path.join(DATA_DIR, 'test.csv'), header=0)\n",
    "df_train = pd.read_csv(os.path.join(DATA_DIR, 'train.csv'), header=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* check nans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check nans\n",
    "print(f'df_test nans: \\n{nans(df_test)}\\n')\n",
    "print(f'df_train nans: \\n{nans(df_train)}\\n')\n",
    "\n",
    "df_test_copy = df_test.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we go on to next section. lets check if there is abnormal distribution in train dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(df_train['target'], kde=True).set(title=\"HistPlot of Train Target\")\n",
    "unexpected = (df_train['target'] < -30).sum()\n",
    "print(f'unexpected value: {unexpected}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly there exists some unexpected values when target < -30.\n",
    "\n",
    "\n",
    "One more thing to notice is the distributiton is symmetric to 0. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_copy = df_train[df_train['target'] > -30]\n",
    "sns.histplot(df_train_copy['target'], kde=True).set(title=\"HistPlot of Train Target\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "then we check if the id is unique, this procedure is important as we might need to outer join other datasets with id. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_count = df_train_copy.shape[0]\n",
    "test_count = df_test_copy.shape[0]\n",
    "print(df_train_copy['card_id'].nunique() == train_count)\n",
    "print(df_test_copy['card_id'].nunique() == test_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_copy.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have to convert obejct to numeric values, as we seen above, there are two object values, since the card_id is a foreign key bound to match more features in other dataset, we'll leave it intact. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "features = ['first_active_month']\n",
    "df_temps = reformat_dataframe(df_test_copy.append(df_train_copy), features, change_object_col)\n",
    "df_test_copy, df_train_copy = to_tuple(split_with_index(df_temps, test_count))\n",
    "del df_temps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "After change the columns we would like to see if the train and test features are evenly splitted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['first_active_month', 'feature_1', 'feature_2', 'feature_3']\n",
    "for feature in features:\n",
    "    (df_train_copy[feature].value_counts().sort_index() / train_count).plot()\n",
    "    (df_test_copy[feature].value_counts().sort_index() / test_count).plot()\n",
    "    plt.legend(['train', 'test'])\n",
    "    plt.xlabel(feature)\n",
    "    plt.ylabel('ratio')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to standarize the features, since not every model need standarized data, we will just leave the builder here uncollected.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "builder = StreamerBuilder.build([df_test_copy, df_train_copy]) \\\n",
    "    .map(lambda df: reformat_dataframe(df, features, standarize_col))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last step is to write the result back, and fetch when we needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_copy.to_csv(os.path.join(OUT_DIR, 'test_pre.csv'), index=False)\n",
    "df_train_copy.to_csv(os.path.join(OUT_DIR, 'train_pre.csv'), index=False)\n",
    "\n",
    "del df_test, df_test_copy, df_train, df_train_copy, features, test_count, train_count, unexpected\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merchant Data purge\n",
    "\n",
    "Now we will hava a look at merchants.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merchant = pd.read_csv(os.path.join(DATA_DIR, 'merchants.csv'), header=0)\n",
    "df_merchant.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (df_merchant.shape[0], df_merchant['merchant_id'].nunique())\n",
    "nans(df_merchant)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obviously, there exists some merchant whose id appears multiple times in this dataset. In catagory_2 lacks a significant amount of datas. We will check the unique value from category 4 to see if it is possible to repace nans to 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will change the object columns to numerical ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "merchants_category_cols = ['merchant_id', 'merchant_group_id', 'merchant_category_id',\n",
    "                 'subsector_id', 'category_1', 'most_recent_sales_range', 'most_recent_purchases_range', 'city_id', 'state_id', 'category_4', 'category_2']\n",
    "\n",
    "merchants_numeric_cols = ['numerical_1', 'numerical_2', 'avg_sales_lag3', 'avg_purchases_lag3', 'active_months_lag3', 'avg_sales_lag6', 'avg_purchases_lag6', 'active_months_lag6',\n",
    "                'avg_sales_lag12', 'avg_purchases_lag12', 'active_months_lag12']\n",
    "\n",
    "assert len(merchants_numeric_cols) + len(merchants_category_cols) == len(df_merchant.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "check category columns unique values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merchant[merchants_category_cols].nunique()\n",
    "df_merchant[merchants_category_cols].dtypes\n",
    "df_merchant['category_2'] .unique()\n",
    "df_merchant['category_2'].fillna(-1, inplace=True)\n",
    "cols = ['category_1', 'most_recent_sales_range', 'most_recent_purchases_range', 'category_4']\n",
    "df_merchant = reformat_dataframe(df_merchant, cols, change_object_col)\n",
    "print(df_merchant[df_merchant['category_2'] == -1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merchant[merchants_numeric_cols].dtypes\n",
    "nans(df_merchant)\n",
    "df_merchant.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inf_cols = ['avg_purchases_lag3', 'avg_purchases_lag6', 'avg_purchases_lag12']\n",
    "\n",
    "# replace infinity with second largest values\n",
    "df_merchant = reformat_dataframe(df_merchant, inf_cols, lambda df: df.replace(np.inf, df[df != np.inf].max()))\n",
    "df_merchant.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill nans with mean values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merchant = reformat_dataframe(df_merchant, merchants_numeric_cols, lambda df: df.fillna(df.mean()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# History Transaction and New Merchant Transaction Data Purge\n",
    "\n",
    "These two datasets will be purged all together bacause they are highly relevant. In order to process the datasets efficiently we will use a StreamerBuilder to process the data chunk by chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "chunksize=10 ** 6\n",
    "\n",
    "def histories_builder() -> StreamerBuilder[pd.DataFrame] :\n",
    "    return StreamerBuilder.build(pd.read_csv(os.path.join(DATA_DIR, \"historical_transactions.csv\"),\n",
    "                       chunksize=chunksize))\n",
    "\n",
    "def append_df(df1, df2):\n",
    "    return df1.append(df2)\n",
    "\n",
    "def sum_df(df1, df2):\n",
    "    return df1.add(df2)\n",
    "\n",
    "df_new_merchants = pd.read_csv(os.path.join(DATA_DIR, \"new_merchant_transactions.csv\"))\n",
    "df_new_merchants.info()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will firstly check the dulplciate columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicate_cols = []\n",
    "\n",
    "for col in df_new_merchants.columns:\n",
    "    if col in df_merchant.columns:\n",
    "        duplicate_cols.append(col)\n",
    "\n",
    "duplicate_cols = ['merchant_id', 'city_id', 'category_1', 'merchant_category_id', 'category_2', 'state_id', 'subsector_id']\n",
    "df_merchant = df_merchant.drop(duplicate_cols[1:], axis=1)\n",
    "df_merchant = df_merchant.loc[df_merchant['merchant_id'].drop_duplicates().index]\n",
    "df_merchant.to_csv(os.path.join(OUT_DIR, 'merchants_pre.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new_merchants[duplicate_cols].drop_duplicates().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new_merchants['merchant_id'].nunique()\n",
    "cols = ['most_recent_sales_range', 'most_recent_purchases_range', 'merchant_id']\n",
    "df_new_merchants = df_new_merchants.merge(df_merchant[cols], how='left', on='merchant_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions_numeric = ['month_lag', 'installments', 'purchase_amount']\n",
    "transactions_category = ['card_id', 'authorized_flag', 'category_3', 'category_1', 'merchant_category_id', 'subsector_id', 'merchant_id', 'city_id', \"state_id\", 'category_2', 'most_recent_sales_range', 'most_recent_purchases_range']\n",
    "# reserved for time series model\n",
    "transactions_time_cols = ['purchase_date']\n",
    "\n",
    "\n",
    "assert len(df_new_merchants.columns) == len(transactions_category) + len(transactions_numeric) + len(transactions_time_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new_merchants[transactions_category].dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nans(df_new_merchants)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['authorized_flag', 'category_1', 'category_3']\n",
    "df_new_merchants = reformat_dataframe(df_new_merchants, features, change_object_col)\n",
    "\n",
    "\n",
    "# convert dtype to small int\n",
    "df_new_merchants['category_2'].fillna(-1, inplace=True)\n",
    "df_new_merchants['category_2'] = df_new_merchants['category_2'].astype(np.int8)\n",
    "df_new_merchants['category_1'] = df_new_merchants['category_1'].astype(np.int8)\n",
    "df_new_merchants['authorized_flag'] = df_new_merchants['authorized_flag'].astype(np.int8)\n",
    "df_new_merchants['installments'] = df_new_merchants['installments'].astype(np.int8)\n",
    "df_new_merchants['state_id'] = df_new_merchants['state_id'].astype(np.int8)\n",
    "df_new_merchants['city_id'] = df_new_merchants['city_id'].astype(np.int8)\n",
    "df_new_merchants['subsector_id'] = df_new_merchants['subsector_id'].astype(np.int8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert purchase date to timeseries\n",
    "datetime_index = pd.DatetimeIndex(df_new_merchants['purchase_date'])\n",
    "df_new_merchants.info()\n",
    "transactions_dtype = df_new_merchants.dtypes\n",
    "df_new_merchants['purchase_day'] = datetime_index.day\n",
    "df_new_merchants['purchase_month'] = datetime_index.month\n",
    "df_new_merchants['purchase_year'] = datetime_index.year\n",
    "df_new_merchants['purchase_hour_section'] = datetime_index.time\n",
    "\n",
    "transactions_features = df_new_merchants.columns\n",
    "transactions_path = os.path.join(OUT_DIR, 'transactions_pre.csv')\n",
    "if os.path.exists(transactions_path):\n",
    "    os.remove(transactions_path)\n",
    "df_new_merchants.to_csv(transactions_path, mode='a', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is no need to fill numerical columns since they don't have nans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reserved for large memory write-ins\n",
    "del df_new_merchants, datetime_index\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "histories_nans = histories_builder().map(nans).reduce(sum_df).collect(identity)\n",
    "print(histories_nans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can discover the history columns and new merchants columns are the same, so we can easily using the above methods again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_datetime_index(df: pd.DataFrame):\n",
    "    _datetime_index = pd.DatetimeIndex(df['purchase_date'])\n",
    "    df['purchase_day'] = _datetime_index.day\n",
    "    df['purchase_month'] = _datetime_index.month\n",
    "    df['purchase_year'] = _datetime_index.year\n",
    "    df['purchase_hour_section'] = _datetime_index.time\n",
    "    return df\n",
    "\n",
    "def convert_columns(df: pd.DataFrame):\n",
    "    df['category_2'].fillna(-1, inplace=True)\n",
    "    return df\n",
    "\n",
    "to_csv_builder = histories_builder() \\\n",
    "                    .map(lambda df: reformat_dataframe(df, features, change_object_col)) \\\n",
    "                    .map(lambda df: df.merge(df_merchant[cols], how='left', on='merchant_id')) \\\n",
    "                    .map(convert_columns) \\\n",
    "                    .map(add_datetime_index)\n",
    "\n",
    "# write to csv\n",
    "to_csv_builder.consume(lambda df: df.to_csv(transactions_path, mode='a', index=False, header=False))\n",
    "\n",
    "del to_csv_builder\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.2 Features Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRE_DIR = os.path.join(DATA_DIR, \"pre\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will extract each columns by their numerical features and append them to the end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_aggs_cols() -> t.Tuple[t.Dict[str, str], t.List[str]]:\n",
    "    aggs = {}\n",
    "    for col in transactions_numeric:\n",
    "        aggs[col] = ['nunique', 'mean', 'min', 'max', 'var', 'skew', 'sum']\n",
    "    for col in transactions_category:\n",
    "        aggs[col] = ['nunique']\n",
    "    aggs['card_id'] = ['size', 'count']\n",
    "    cols = []\n",
    "    for key in aggs.keys():\n",
    "        cols.extend([key+\"_\"+stat for stat in aggs[key]])\n",
    "    return aggs, cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@enable_logging(\"append_new_columns.log\")\n",
    "def append_new_columns(transaction, aggs, cols):\n",
    "    df = transaction[transaction['month_lag'] < 0].groupby('card_id').agg(aggs)\n",
    "    df.columns = [co + \"_hist\" for co in cols]\n",
    "    df2 = transaction[transaction['month_lag'] >= 0].groupby('card_id').agg(aggs)\n",
    "    df2.columns =[co + \"_hist\" for co in cols]\n",
    "    df = df.reset_index()\n",
    "    df2 = df2.reset_index()\n",
    "    df = df.merge(df2, how='left', on='card_id')\n",
    "    df2 = transaction.groupby('card_id').agg(aggs)\n",
    "    df2.columns = cols\n",
    "    df2 = df2.reset_index()\n",
    "    if not df.empty:\n",
    "        df = df.merge(df2, how='left', on='card_id')\n",
    "    else:\n",
    "        df = df2.copy()\n",
    "    del transaction\n",
    "    gc.collect()\n",
    "    return df\n",
    "\n",
    "@enable_logging(\"write_to_csv.log\")\n",
    "def write_to_csv(df):\n",
    "    train = pd.read_csv(os.path.join(PRE_DIR, \"train_pre.csv\"))\n",
    "    train = train.merge(df, how='left', on='card_id')\n",
    "    logging.debug(f'Training df after first merge')\n",
    "\n",
    "    test = pd.read_csv(os.path.join(PRE_DIR, \"test_pre.csv\"))\n",
    "    test = test.merge(df, how='left', on='card_id')\n",
    "\n",
    "    train.to_csv(os.path.join(PRE_DIR, \"train_groupby.csv\"), index=False, mode='a')\n",
    "    test.to_csv(os.path.join(PRE_DIR, \"test_groupby.csv\"), index=False, mode='a')\n",
    "    del df\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Following block is extremely time-consuming, run with caution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_groupby, test_groupby = os.path.join(PRE_DIR, \"train_groupby.csv\"), os.path.join(PRE_DIR, \"test_groupby.csv\")\n",
    "if os.path.exists(train_groupby):\n",
    "    os.remove(train_groupby)\n",
    "if os.path.exists(test_groupby):\n",
    "    os.remove(test_groupby)\n",
    "\n",
    "aggs, cols = create_aggs_cols()\n",
    "\n",
    "dtype = transactions_dtype.to_dict()\n",
    "\n",
    "# we will use dtypes collected from previous to save memory\n",
    "transactions = pd.read_csv(os.path.join(PRE_DIR, \"transactions_pre.csv\"), dtype=dtype)\n",
    "transactions = append_new_columns(transactions, aggs, cols)\n",
    "write_to_csv(transactions)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ccbb4ca7fe902b23c4d3bf398cc69bd54ad114cdf2cf797081a3177c4f0863aa"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
